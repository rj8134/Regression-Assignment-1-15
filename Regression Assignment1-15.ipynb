{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48b1347e-260e-4321-bb14-95ad00a07c1f",
   "metadata": {},
   "source": [
    "# Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3526d0be-0e4e-47b9-920b-f4b17f06707a",
   "metadata": {},
   "source": [
    "Linear regression is a supervised machine learning algorithm that is used to predict a continuous value, such as price, profit, or weight, based on a set of independent variables. The basic concept of linear regression is to find a line that best fits the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1aa41d-7cdd-449e-bdde-dc9c27858c18",
   "metadata": {},
   "source": [
    "There are three main types of linear regression:\n",
    "Simple linear regression\n",
    "Multiple linear regression\n",
    "Polynomial linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2925dc4e-9e8a-4265-8e1b-1b52d73f49d8",
   "metadata": {},
   "source": [
    "Simple Linear Regression:\n",
    "Important to understand but will never come in real life scenario. It has only one independent variable. This means that the dependent variable is modeled as a linear function of the independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbedd00f-9689-42e6-b5c7-70d9a5038986",
   "metadata": {},
   "source": [
    "Multiple linear regression has multiple independent variables. This means that the dependent variable is modeled as a linear function of multiple independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9e81aa-34cc-4189-9114-ae7a4c51a13a",
   "metadata": {},
   "source": [
    "Polynomial linear regression is a special case of multiple linear regression where the independent variables are raised to different powers. This allows the model to fit non-linear relationships between the dependent and independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae8c20c-8510-465e-a32f-e9098e52820c",
   "metadata": {},
   "source": [
    "Simple linear regression has only one X and one y variable.\n",
    "\n",
    "Multiple linear regression has one y and two or more X variables.\n",
    "\n",
    "For instance, when we predict rent based on square feet alone that is simple linear regression.\n",
    "\n",
    "When we predict rent based on square feet and age of the building that is an example of multiple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3385e75a-b594-41c2-ab06-52809e343dda",
   "metadata": {},
   "source": [
    "# Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0a70be-5516-46c6-bbd3-da1fcb970dd6",
   "metadata": {},
   "source": [
    "There are primarily five assumptions of linear regression. They are:\n",
    "\n",
    "There is a linear relationship between the predictors (x) and the outcome (y)\n",
    "Predictors (x) are independent and observed with negligible error\n",
    "Residual Errors have a mean value of zero\n",
    "Residual Errors have constant variance\n",
    "Residual Errors are independent from each other and predictors (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247a2336-fe4c-41b5-8353-ddd2bcd7a3b9",
   "metadata": {},
   "source": [
    "# Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46526f27-8df9-40d9-bb08-0d8182bb4481",
   "metadata": {},
   "source": [
    "Y-intercept is the point at which the line intersects the y-axis at x = 0. It is also the value the model would take or predict when x is 0.\n",
    "\n",
    "Coefficients provide the impact or weight of a variable towards the entire model. In other words, it provides the amount of change in the dependent variable for a unit change in the independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb55274-1688-4c96-a54f-bd5c70916d4b",
   "metadata": {},
   "source": [
    "price of the car=b0 + 2.5*Model of the car\n",
    "if there is one unit change in model of the car,price of the car will increase by 2.5 units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cf979f-84b0-4cd9-a6ac-0297b36fddae",
   "metadata": {},
   "source": [
    "# Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068e83dc-7512-4515-a7be-259c77a9593d",
   "metadata": {},
   "source": [
    "Gradient descent is an optimization algorithm used in machine learning to minimize the cost function by iteratively adjusting parameters in the direction of the negative gradient, aiming to find the optimal set of parameters.\n",
    "\n",
    "The cost function represents the discrepancy between the predicted output of the model and the actual output. The goal of gradient descent is to find the set of parameters that minimizes this discrepancy and improves the model’s performance.\n",
    "\n",
    "The algorithm operates by calculating the gradient of the cost function, which indicates the direction and magnitude of steepest ascent. However, since the objective is to minimize the cost function, gradient descent moves in the opposite direction of the gradient, known as the negative gradient direction.\n",
    "\n",
    "By iteratively updating the model’s parameters in the negative gradient direction, gradient descent gradually converges towards the optimal set of parameters that yields the lowest cost. The learning rate, a hyperparameter, determines the step size taken in each iteration, influencing the speed and stability of convergence.\n",
    "\n",
    "Gradient descent can be applied to various machine learning algorithms, including linear regression, logistic regression, neural networks, and support vector machines. It provides a general framework for optimizing models by iteratively refining their parameters based on the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f4d23b-195d-489b-8f6d-6b110b6a9829",
   "metadata": {},
   "source": [
    "How Does Gradient Descent Work?\n",
    "Gradient descent is an optimization algorithm used to minimize the cost function of a model.\n",
    "The cost function measures how well the model fits the training data and is defined based on the difference between the predicted and actual values.\n",
    "The gradient of the cost function is the derivative with respect to the model’s parameters and points in the direction of the steepest ascent.\n",
    "The algorithm starts with an initial set of parameters and updates them in small steps to minimize the cost function.\n",
    "In each iteration of the algorithm, the gradient of the cost function with respect to each parameter is computed.\n",
    "The gradient tells us the direction of the steepest ascent, and by moving in the opposite direction, we can find the direction of the steepest descent.\n",
    "The size of the step is controlled by the learning rate, which determines how quickly the algorithm moves towards the minimum.\n",
    "The process is repeated until the cost function converges to a minimum, indicating that the model has reached the optimal set of parameters.\n",
    "There are different variations of gradient descent, including batch gradient descent, stochastic gradient descent, and mini-batch gradient descent, each with its own advantages and limitations.\n",
    "Efficient implementation of gradient descent is essential for achieving good performance in machine learning tasks. The choice of the learning rate and the number of iterations can significantly impact the performance of the algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1946ac4e-7d53-4b7b-a240-6f37a8676d91",
   "metadata": {},
   "source": [
    "# Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e865ad84-92cf-4a88-a0c7-361d800bc7dd",
   "metadata": {},
   "source": [
    "Multiple linear regression is used to estimate the relationship between two or more independent variables and one dependent variable. You can use multiple linear regression when you want to know:\n",
    "\n",
    "How strong the relationship is between two or more independent variables and one dependent variable (e.g. how rainfall, temperature, and amount of fertilizer added affect crop growth).\n",
    "The value of the dependent variable at a certain value of the independent variables (e.g. the expected yield of a crop at certain levels of rainfall, temperature, and fertilizer addition)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c193244b-1840-4f28-ae5a-dee9770b5d74",
   "metadata": {},
   "source": [
    "Multiple linear regression example\n",
    "You are a public health researcher interested in social factors that influence heart disease. You survey 500 towns and gather data on the percentage of people in each town who smoke, the percentage of people in each town who bike to work, and the percentage of people in each town who have heart disease.\n",
    "Because you have two independent variables and one dependent variable, and all your variables are quantitative, you can use multiple linear regression to analyze the relationship between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aabbc1a-cd91-4f79-8176-1496dcccb09c",
   "metadata": {},
   "source": [
    "A multiple regression model is a linear regression model that has been expanded to include more than one independent variable. By logic, this means it performs better than a simple regression. Multiple regressions are used for: \n",
    "\n",
    "Planning and monitoring\n",
    "Prediction or forecasting. \n",
    "The investigator will use multiple linear regression to account for all of these potentially significant variables in one model. The benefits of this approach can include a more accurate and detailed view of the relationship between each particular factor and the outcome. Another great advantage of multiple linear regression is the application of the multiple regression model in scientific research. Researchers may use multiple regression analysis to evaluate the strength of the relationship between an outcome (the dependent variable) and several predictor variables and the contribution of each predictor to the relationship, often with the influence of other predictors statistically eliminated.\n",
    "\n",
    "So, it depends on what function you need each model to perform when considering linear vs multiple regression models; it’s all a matter of usage in the end. With that out of the way, you should be aware that there are, of course, other matchups that might spark your interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36062538-56a5-4fab-b0b7-8b598f6a6823",
   "metadata": {},
   "source": [
    "# Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e107515e-1d86-4a19-b5be-0d8d9f9947f6",
   "metadata": {},
   "source": [
    "Multicollinearity is a statistical phenomenon that occurs when two or more independent variables in a regression model are highly correlated with each other. In other words, multicollinearity indicates a strong linear relationship among the predictor variables. This can create challenges in the regression analysis because it becomes difficult to determine the individual effects of each independent variable on the dependent variable accurately.\n",
    "\n",
    "Multicollinearity can lead to unstable and unreliable coefficient estimates, making it harder to interpret the results and draw meaningful conclusions from the model. It is essential to detect and address multicollinearity to ensure the validity and robustness of regression models.\n",
    "\n",
    "Multicollinearity occurs when two or more independent variables in a data frame have a high correlation with one another in a regression model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b38a7-38fe-480b-9730-42a07781a972",
   "metadata": {},
   "source": [
    "Causes\n",
    "There are many reasons why multicollinearity may occur.\n",
    "\n",
    "It may occur as a result of:\n",
    "\n",
    "The inclusion of identical variables. For example, one may have identical variables in a dataset, such as mass in kilograms and mass in pounds.\n",
    "\n",
    "Creation of new variables that are dependent on others. When we create variables that depend on other variables, we introduce redundant information to the model. As such, we may inadvertently encourage the occurrence of multicollinearity.\n",
    "\n",
    "Inadequate data. In some cases, when the data is inadequate, we may experience multicollinearity. This is due to the small sample size, which might, in turn, experience great variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56db611e-3197-48f3-9f64-a3a3648f7d74",
   "metadata": {},
   "source": [
    "Addressing multicollinearity\n",
    "If we conclude that multicollinearity poses a problem for our regression model, we can attempt a handful of basic fixes.\n",
    "\n",
    "Removing variables. A straightforward method of correcting multicollinearity is removing one or more variables showing a high correlation. This assists in reducing the multicollinearity linking correlated features. It is advisable to get rid of variables iteratively. We would begin with a variable with the highest VIF score since other variables are likely to capture its trend. As a result of removing this variable, other variables’ VIF values are likely to reduce.\n",
    "\n",
    "More data. Statistically, a regression model with more data is likely to suffer less variance due to a larger sample size. This will reduce the impact of multicollinearity.\n",
    "\n",
    "Using techniques such as partial least squares regression (PLS) and principal component analysis (PCA). A takeaway from this paper on partial least squares regression for multicollinearity is that PLS can lessen variables to a smaller grouping with no correlation between them. PLS, like PCA, is a dimensionality reduction technique. PCA reduces the dimension of data through the decomposition of data into independent factors. Therefore, new variables with no correlation between them are created. This article explains how PCA handles multicollinearity.\n",
    "\n",
    "Centering the variables. Centering is defined as subtracting a constant from the value of every variable. It redefines the zero point for a given predictor to become the value we subtracted. Here’s how we can center the variables. We can first calculate the mean of every single independent variable. The next step would replace each variable value with the difference between the values and the mean. The result of this is that the interpretation of the regression coefficients remains unchanged while reducing the impact of multicollinearity by making the correlation between variables much more manageable. This post contains an example of how centered variables lead to reduced multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f7abb4-d282-420e-927e-3906181e179c",
   "metadata": {},
   "source": [
    "# Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba88a1ec-caec-4e07-87c1-f7728581901e",
   "metadata": {},
   "source": [
    "Polynomial regression is a particular case of linear regression where a polynomial regression is fit into the data with the help of a curvilinear relationship shared by the independent variables and the target variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9859b7e-4095-4f26-9ce6-55411cc74cd4",
   "metadata": {},
   "source": [
    "A polynomial regression model is a machine learning model that can capture non-linear relationships between variables by fitting a non-linear regression line, which may not be possible with simple linear regression.\n",
    "It is used when linear regression models may not adequately capture the complexity of the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd71dd74-62ad-4973-8635-afe835bcb04a",
   "metadata": {},
   "source": [
    "where data points are arranged in a non-linear fashion, we need the Polynomial Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65783d55-b188-4724-bbaf-c82e880af918",
   "metadata": {},
   "source": [
    "Linear regression relates two variables with a straight line; nonlinear regression relates the variables using a curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eb1bbf-8b00-4406-95bf-29d8110ee3ef",
   "metadata": {},
   "source": [
    "# Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3564113-1427-4556-addc-66b197eb362b",
   "metadata": {},
   "source": [
    "Advantage – Polynomial Regression\n",
    "The polynomial regression is flexible enough to get fitted in a vast range of curvatures. \n",
    "A broad range of functions can easily fit under it. \n",
    "The polynomial regression offers the best approximation of the relationship between the two dependent and independent variables. \n",
    "Disadvantage – Polynomial Regression\n",
    "The presence of one or more outliers in the data can hurt the final results of the nonlinear analysis. \n",
    "The polynomial regression is very sensitive to the outliers. \n",
    "Very few model validation tools are available that help detect the outliers in nonlinear regression compared to the ones present for linear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501947a0-4461-44c7-997c-a9d377ba2082",
   "metadata": {},
   "source": [
    "Polynomial regression is a simple yet powerful tool for predictive analytics. It allows you to consider non-linear relations between variables and reach conclusions that can be estimated with high accuracy. This type of regression can help you predict disease spread rate, calculate fair compensation, or implement a preventative road safety regulation software."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3633c8-183b-4cff-9501-9787e1c5cd83",
   "metadata": {},
   "source": [
    "It is used when linear regression models may not adequately capture the complexity of the relationship."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
